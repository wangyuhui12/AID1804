

机器学习(learning machine)致力于研究如何通过计算的手段，利用经验来改善系统自身的性能。

从机器学习所研究的主要内容，是关于在计算机上从数据中产生“模型”(model)的算法，即“学习算法”(learning algorithm)

模型泛指从数据中学得的结果

基本术语：

从数据中学得模型的过程称为“学习(learning)”或“训练(training)”，这个过程通过执行某个学习算法来完成。
训练过程中的数据称为“训练数据”(training data)，其中每个样本称为一个“训练样本”(training sample).

学得模型对应了关于数据的某种潜在的规律，称为“假设”(hypothesis)

样例(example)：拥有了标记信息的示例。所有标记的集合，称为“标记空间”(label space)　或“输出空间”


根据训练数据是否拥有标记信息，学习任务可大致分为两大类：
    监督学习(supervised learning)
    无监督学习(unsupervised learning)
分类回归是前者的代表。聚类则是后者的代表。

学得模型适用于新样本的能力，称为“泛化”(generalization)能力。

“从样例中学习”显然是一个归纳的过程，因此亦称为“归纳学习”(inductive learning)

归纳学习有狭义和广义之分，广义的归纳学习大体相当于从样例中学习，而狭义的归纳学习则要求从训练数据中学得概念(concept)，因此亦称为“概念学习”或“概念形成”。

机器学习算法在学习过程中对某种类型假设的偏好，称为“归纳偏好”(inductive bias),或简称为“偏好”
归纳偏好可看做学习算法自身在一个可能很庞大的假设空间中对假设进行选择的启发式或“价值观”。

尽可能特殊即“适用情形尽可能少”
尽可能一般即“适用情形尽可能多”

特征选择(feature selection)和属性选择有关。机器学习中的特征选择是基于对训练样本的分析进行的。


一般性的原则来引导算法确立“正确的”偏好：
“奥卡姆剃刀”(Occam's razor)是一种常用的、自然科学研究中最基本的原则，即“若有多个假设与观察一致，则选最简单的那个”。

事实上，归纳偏好对应了学习算法本身所做出的关于“什么样的模型更好”的假设，在具体的显示问题中，这个假设是否成立，即算法的归纳偏好是否与问题本身匹配，大多时候直接决定了算法能否取得好的性能。


NFL定理(No Free Lunch Theorem 定理)  "没有免费的午餐"定理：所有学习算法的期望性能相同。
NFL定理的重要前提：　所有“问题”出现的机会相同、或所有问题同等重要。
NFL定理最重要的寓意，是让我们清楚地认识到，脱离具体问题，空泛地谈论“什么学习算法更好”毫无意义，因为若考虑所有潜在的问题，则所有学习算法一样好。

”从样例中学习“的一大主流是符号主义学习，其代表包括决策树(decision tree)和基于逻辑的学习。
典型的决策树学习以信息论为基础，以信息熵为最小化目标，直接模拟了人类对概念进行判定的树形流程。
基于逻辑的学习的著名代表是归纳逻辑程序设计(Inductive Logic Programming, 简称ILP)，可看做机器学习与逻辑程序设计的交叉，它使用一阶逻辑（即谓词逻辑）来进行知识表示，
通过修改和扩充逻辑表达式(例如Prolog表达式)来完成对数据的归纳。
ILP具有很强的知识表示能力，可以较容易地表达出复杂数据关系，而且领域知识通常可方便地通过逻辑表达式进行描述。
ILP不仅可利用领域知识辅助学习，还可通过学习对领域知识进行精化和增强。
缺点：表示能力太强导致难以有效进行学习。

”从样例中学习“（广义的归纳学习）的另一主流技术是基于神经网络的连接主义学习。产生的是”黑箱“模型。

“统计学习”(statistical learning)　代表技术是支持向量机(Support Vector Machine, 简称SVM)以及更一般的“核方法”(kernel methods).

深度学习，“很多层”的神经网络。

机器学习提供数据分析能力，云计算提供数据处理能力，众包提供数据标记能力。




